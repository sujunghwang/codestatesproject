{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (4.17.0)\n",
      "Requirement already satisfied: pytorch_lightning in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (1.5.10)\n",
      "Requirement already satisfied: seqeval in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: omegaconf in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (0.1.96)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from pytorch_lightning) (2022.2.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from pytorch_lightning) (0.7.2)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from pytorch_lightning) (2.8.0)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: setuptools==59.5.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from pytorch_lightning) (59.5.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from seqeval) (1.0.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from omegaconf) (4.8)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.6.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
      "Requirement already satisfied: six in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\mcr\\.conda\\envs\\cuda\\lib\\site-packages (from sacremoses->transformers) (8.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers pytorch_lightning seqeval omegaconf sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "n_devices = torch.cuda.device_count()\n",
    "print(n_devices)\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def load_slot_labels() -> List[str]:\n",
    "    \"\"\"tag label 종류 리턴\"\"\"\n",
    "    return [\"UNK\", \"PAD\", \"O\", \"B\", \"I\", \"E\", \"S\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, data_path: str, transform: Callable[[List, List], Tuple]):\n",
    "        self.sentences = []\n",
    "        self.transform = transform\n",
    "        self.slot_labels = load_slot_labels()\n",
    "\n",
    "        self._load_data(data_path)\n",
    "\n",
    "    def _load_data(self, data_path: str):\n",
    "        \"\"\"data를 file에서 불러온다.\n",
    "        Args:\n",
    "            data_path: file 경로\n",
    "        \"\"\"\n",
    "        with open(data_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            self.sentences = [line.split() for line in lines]\n",
    "\n",
    "    def _get_tags(self, sentence: List[str]) -> List[str]:\n",
    "        \"\"\"문장에 대해 띄어쓰기 tagging을 한다.\n",
    "        character 단위로 분리하여 BIES tagging을 한다.\n",
    "        Args:\n",
    "            sentence: 문장\n",
    "        Retrns:\n",
    "            문장의 각 토큰에 대해 tagging한 결과 리턴\n",
    "            [\"B\", \"I\", \"E\"]\n",
    "        \"\"\"\n",
    "\n",
    "        all_tags = []\n",
    "        for word in sentence:\n",
    "            if len(word) == 1:\n",
    "                all_tags.append(\"S\")\n",
    "            elif len(word) > 1:\n",
    "                for i, c in enumerate(word):\n",
    "                    if i == 0:\n",
    "                        all_tags.append(\"B\")\n",
    "                    elif i == len(word) - 1:\n",
    "                        all_tags.append(\"E\")\n",
    "                    else:\n",
    "                        all_tags.append(\"I\")\n",
    "        return all_tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = \"\".join(self.sentences[idx])\n",
    "        tags = self._get_tags(self.sentences[idx])\n",
    "        tags = [self.slot_labels.index(t) for t in tags]\n",
    "\n",
    "        (\n",
    "            input_ids,\n",
    "            slot_labels,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "        ) = self.transform(sentence, tags)\n",
    "\n",
    "        return input_ids, slot_labels, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_kobert import KoBertTokenizer\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, max_len: int):\n",
    "        self.tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "        self.max_len = max_len\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "    def get_input_features(\n",
    "        self, sentence: List[str], tags: List[str]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"문장과 띄어쓰기 tagging에 대해 feature로 변환한다.\n",
    "\n",
    "        Args:\n",
    "            sentence: 문장\n",
    "            tags: 띄어쓰기 tagging\n",
    "\n",
    "        Returns:\n",
    "            feature를 리턴한다.\n",
    "            input_ids, attention_mask, token_type_ids, slot_labels\n",
    "        \"\"\"\n",
    "\n",
    "        input_tokens = []\n",
    "        slot_label_ids = []\n",
    "        \n",
    "        # tokenize\n",
    "        for word, tag in zip(sentence, tags):\n",
    "            tokens = self.tokenizer.tokenize(word)\n",
    "\n",
    "            if len(tokens) == 0:\n",
    "                tokens = self.tokenizer.unk_token\n",
    "\n",
    "            input_tokens.extend(tokens)\n",
    "\n",
    "            for i in range(len(tokens)):\n",
    "                if i == 0:\n",
    "                    slot_label_ids.extend([tag])\n",
    "                else:\n",
    "                    slot_label_ids.extend([self.pad_token_id])\n",
    "\n",
    "        # max_len보다 길이가 길면 뒤에 자르기\n",
    "        if len(input_tokens) > self.max_len - 2:\n",
    "            input_tokens = input_tokens[: self.max_len - 2]\n",
    "            slot_label_ids = slot_label_ids[: self.max_len - 2]\n",
    "\n",
    "        # cls, sep 추가\n",
    "        input_tokens = (\n",
    "            [self.tokenizer.cls_token] + input_tokens + [self.tokenizer.sep_token]\n",
    "        )\n",
    "        slot_label_ids = [self.pad_token_id] + slot_label_ids + [self.pad_token_id]\n",
    "\n",
    "        # token을 id로 변환\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        # padding\n",
    "        pad_len = self.max_len - len(input_tokens)\n",
    "        input_ids = input_ids + ([self.tokenizer.pad_token_id] * pad_len)\n",
    "        slot_label_ids = slot_label_ids + ([self.pad_token_id] * pad_len)\n",
    "        attention_mask = attention_mask + ([0] * pad_len)\n",
    "        token_type_ids = token_type_ids + ([0] * pad_len)\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long)\n",
    "        slot_label_ids = torch.tensor(slot_label_ids, dtype=torch.long)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids, slot_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertConfig, BertModel, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "\n",
    "class SpacingBertModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        ner_train_dataloader: DataLoader,\n",
    "        ner_val_dataloader: DataLoader,\n",
    "        ner_test_dataloader: DataLoader,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.ner_train_dataloader = ner_train_dataloader\n",
    "        self.ner_val_dataloader = ner_val_dataloader\n",
    "        self.ner_test_dataloader = ner_test_dataloader\n",
    "        self.slot_labels_type = load_slot_labels()\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "        self.bert_config = BertConfig.from_pretrained(\n",
    "            self.config.bert_model, num_labels=len(self.slot_labels_type)\n",
    "        )\n",
    "        self.model = BertModel.from_pretrained(\n",
    "            self.config.bert_model, config=self.bert_config\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.config.dropout_rate)\n",
    "        self.linear = nn.Linear(\n",
    "            self.bert_config.hidden_size, len(self.slot_labels_type)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        x = outputs[0]\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "\n",
    "        input_ids, attention_mask, token_type_ids, slot_labels = batch\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        loss = self._calculate_loss(outputs, slot_labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "\n",
    "        input_ids, attention_mask, token_type_ids, slot_labels = batch\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        loss = self._calculate_loss(outputs, slot_labels)\n",
    "        self.log('val_loss', loss)\n",
    "        gt_slot_labels, pred_slot_labels = self._convert_ids_to_labels(\n",
    "            outputs, slot_labels\n",
    "        )\n",
    "\n",
    "        val_acc = self._f1_score(gt_slot_labels, pred_slot_labels)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_acc\": val_acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        val_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_log = {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "        }\n",
    "\n",
    "        return {\"val_loss\": val_loss, \"progress_bar\": tensorboard_log}\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "\n",
    "        input_ids, attention_mask, token_type_ids, slot_labels = batch\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        gt_slot_labels, pred_slot_labels = self._convert_ids_to_labels(\n",
    "            outputs, slot_labels\n",
    "        )\n",
    "\n",
    "        test_acc = self._f1_score(gt_slot_labels, pred_slot_labels)\n",
    "\n",
    "        test_step_outputs = {\n",
    "            \"test_acc\": test_acc,\n",
    "            \"gt_labels\": gt_slot_labels,\n",
    "            \"pred_labels\": pred_slot_labels,\n",
    "        }\n",
    "\n",
    "        return test_step_outputs\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "\n",
    "        gt_labels = []\n",
    "        pred_labels = []\n",
    "        for x in outputs:\n",
    "            gt_labels.extend(x[\"gt_labels\"])\n",
    "            pred_labels.extend(x[\"pred_labels\"])\n",
    "\n",
    "        test_step_outputs = {\n",
    "            \"test_acc\": test_acc,\n",
    "            \"gt_labels\": gt_labels,\n",
    "            \"pred_labels\": pred_labels,\n",
    "        }\n",
    "\n",
    "        return test_step_outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.ner_train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.ner_val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.ner_test_dataloader\n",
    "\n",
    "    def _calculate_loss(self, outputs, labels):\n",
    "        # active_loss = attention_mask.view(-1) == 1\n",
    "        # active_logits = outputs.view(-1, len(self.slot_labels_type))[active_loss]\n",
    "        # active_labels = slot_labels.view(-1)[active_loss]\n",
    "        active_logits = outputs.view(-1, len(self.slot_labels_type))\n",
    "        active_labels = labels.view(-1)\n",
    "        loss = F.cross_entropy(active_logits, active_labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _f1_score(self, gt_slot_labels, pred_slot_labels):\n",
    "        return torch.tensor(\n",
    "            f1_score(gt_slot_labels, pred_slot_labels), dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def _convert_ids_to_labels(self, outputs, slot_labels):\n",
    "        _, y_hat = torch.max(outputs, dim=2)\n",
    "        y_hat = y_hat.detach().cpu().numpy()\n",
    "        slot_label_ids = slot_labels.detach().cpu().numpy()\n",
    "\n",
    "        slot_label_map = {i: label for i, label in enumerate(self.slot_labels_type)}\n",
    "        slot_gt_labels = [[] for _ in range(slot_label_ids.shape[0])]\n",
    "        slot_pred_labels = [[] for _ in range(slot_label_ids.shape[0])]\n",
    "\n",
    "        for i in range(slot_label_ids.shape[0]):\n",
    "            for j in range(slot_label_ids.shape[1]):\n",
    "                if slot_label_ids[i, j] != self.pad_token_id:\n",
    "                    slot_gt_labels[i].append(slot_label_map[slot_label_ids[i][j]])\n",
    "                    slot_pred_labels[i].append(slot_label_map[y_hat[i][j]])\n",
    "\n",
    "        return slot_gt_labels, slot_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = OmegaConf.load(\"c:/users/mcr/desktop/sample/train_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(config.max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    data_path: str, transform: Callable[[List, List], Tuple], batch_size: int\n",
    ") -> DataLoader:\n",
    "    \"\"\"dataloader 생성\n",
    "    Args:\n",
    "        data_path: dataset 경로\n",
    "        transform: input feature로 변환해주는 funciton\n",
    "        batch_size: dataloader batch size\n",
    "    Returns:\n",
    "        dataloader\n",
    "    \"\"\"\n",
    "    dataset = CorpusDataset(data_path, transform)\n",
    "    print(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    print(dataloader)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CorpusDataset object at 0x0000015448806108>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000015448806208>\n",
      "<__main__.CorpusDataset object at 0x0000015448806188>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000154658CEA48>\n",
      "<__main__.CorpusDataset object at 0x0000015448806308>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000154658CEAC8>\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = get_dataloader(\n",
    "    config.train_data_path, preprocessor.get_input_features, config.train_batch_size\n",
    ")\n",
    "val_dataloader = get_dataloader(\n",
    "    config.val_data_path, preprocessor.get_input_features, config.train_batch_size\n",
    ")\n",
    "test_dataloader = get_dataloader(\n",
    "    config.test_data_path, preprocessor.get_input_features, config.eval_batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_finetuner = SpacingBertModel(\n",
    "    config, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir = config.log_path,\n",
    "    version = 1,\n",
    "    name = config.task\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = 'checkpoints',\n",
    "    verbose = True,\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    save_top_k = 1,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 0.001,\n",
    "    patience = 3,\n",
    "    mode = 'min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:898: UserWarning: You are running on single node with no parallelization, so distributed has no effect.\n",
      "  rank_zero_warn(\"You are running on single node with no parallelization, so distributed has no effect.\")\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1585: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  \"GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\"\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "\n",
      "  | Name    | Type      | Params\n",
      "--------------------------------------\n",
      "0 | model   | BertModel | 92.2 M\n",
      "1 | dropout | Dropout   | 0     \n",
      "2 | linear  | Linear    | 5.4 K \n",
      "--------------------------------------\n",
      "92.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "92.2 M    Total params\n",
      "368.769   Total estimated model params size (MB)\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:631: UserWarning: Checkpoint directory c:\\Users\\mcr\\Desktop\\sample\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:433: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\closure.py:36: LightningDeprecationWarning: One of the returned values {'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  f\"One of the returned values {set(extra.keys())} has a `grad_fn`. We will detach it automatically\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [05:17<00:00, 158.85s/it, loss=2.05, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss reached 1.88161 (best 1.88161), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=0-step=0.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [05:13<00:00, 156.69s/it, loss=1.96, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: UNK seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Epoch 1, global step 1: val_loss reached 1.73305 (best 1.73305), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=1-step=1.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2/2 [05:07<00:00, 153.78s/it, loss=1.88, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2: val_loss reached 1.61859 (best 1.61859), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=2-step=2.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [05:06<00:00, 153.23s/it, loss=1.82, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 3: val_loss reached 1.54261 (best 1.54261), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=3-step=3.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2/2 [05:03<00:00, 151.60s/it, loss=1.77, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 4: val_loss reached 1.49432 (best 1.49432), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=4-step=4.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [05:00<00:00, 150.38s/it, loss=1.73, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 5: val_loss reached 1.46260 (best 1.46260), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=5-step=5.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2/2 [05:00<00:00, 150.05s/it, loss=1.69, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 6: val_loss reached 1.44121 (best 1.44121), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=6-step=6.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2/2 [05:01<00:00, 150.97s/it, loss=1.66, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 7: val_loss reached 1.42448 (best 1.42448), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=7-step=7.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2/2 [04:59<00:00, 149.88s/it, loss=1.64, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 8: val_loss reached 1.41113 (best 1.41113), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=8-step=8.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [04:58<00:00, 149.44s/it, loss=1.61, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 9: val_loss reached 1.39507 (best 1.39507), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=9-step=9.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2/2 [05:04<00:00, 152.16s/it, loss=1.59, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 10: val_loss reached 1.37931 (best 1.37931), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=10-step=10.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2/2 [05:07<00:00, 153.65s/it, loss=1.57, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 11: val_loss reached 1.36013 (best 1.36013), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=11-step=11.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2/2 [05:00<00:00, 150.42s/it, loss=1.55, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 12: val_loss reached 1.33896 (best 1.33896), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=12-step=12.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 2/2 [05:09<00:00, 154.69s/it, loss=1.53, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 13: val_loss reached 1.32218 (best 1.32218), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=13-step=13.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2/2 [04:59<00:00, 149.94s/it, loss=1.51, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 14: val_loss reached 1.30253 (best 1.30253), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=14-step=14.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 2/2 [05:02<00:00, 151.29s/it, loss=1.49, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 15: val_loss reached 1.27878 (best 1.27878), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=15-step=15.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 2/2 [05:05<00:00, 152.81s/it, loss=1.47, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 16: val_loss reached 1.25221 (best 1.25221), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=16-step=16.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 2/2 [05:04<00:00, 152.38s/it, loss=1.45, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 17: val_loss reached 1.22431 (best 1.22431), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=17-step=17.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 2/2 [05:06<00:00, 153.33s/it, loss=1.43, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 18: val_loss reached 1.19718 (best 1.19718), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=18-step=18.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [05:02<00:00, 151.18s/it, loss=1.41, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 19: val_loss reached 1.16978 (best 1.16978), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=19-step=19.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 2/2 [05:00<00:00, 150.25s/it, loss=1.36, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 20: val_loss reached 1.14483 (best 1.14483), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=20-step=20.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 2/2 [05:00<00:00, 150.35s/it, loss=1.32, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 21: val_loss reached 1.11981 (best 1.11981), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=21-step=21.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 2/2 [04:59<00:00, 149.96s/it, loss=1.28, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 22: val_loss reached 1.09386 (best 1.09386), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=22-step=22.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 2/2 [04:59<00:00, 149.74s/it, loss=1.24, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 23: val_loss reached 1.06751 (best 1.06751), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=23-step=23.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 2/2 [05:01<00:00, 150.95s/it, loss=1.21, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 24: val_loss reached 1.04274 (best 1.04274), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=24-step=24.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 2/2 [05:01<00:00, 150.85s/it, loss=1.17, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 25: val_loss reached 1.02248 (best 1.02248), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=25-step=25.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 2/2 [05:01<00:00, 150.82s/it, loss=1.14, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 26: val_loss reached 1.00621 (best 1.00621), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=26-step=26.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 2/2 [05:09<00:00, 154.57s/it, loss=1.1, v_num=1] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 27: val_loss reached 0.98914 (best 0.98914), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=27-step=27.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 2/2 [05:10<00:00, 155.19s/it, loss=1.07, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 28: val_loss reached 0.96973 (best 0.96973), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=28-step=28.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 2/2 [05:05<00:00, 152.70s/it, loss=1.03, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 29: val_loss reached 0.94954 (best 0.94954), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=29-step=29.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 2/2 [05:08<00:00, 154.49s/it, loss=0.992, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 30: val_loss reached 0.93027 (best 0.93027), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=30-step=30.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 2/2 [05:08<00:00, 154.17s/it, loss=0.956, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 31: val_loss reached 0.90883 (best 0.90883), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=31-step=31.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 2/2 [05:04<00:00, 152.43s/it, loss=0.919, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 32: val_loss reached 0.88956 (best 0.88956), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=32-step=32.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 2/2 [05:00<00:00, 150.29s/it, loss=0.881, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 33: val_loss reached 0.88079 (best 0.88079), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=33-step=33.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 2/2 [05:00<00:00, 150.30s/it, loss=0.845, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 34: val_loss reached 0.87686 (best 0.87686), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=34-step=34.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 2/2 [04:59<00:00, 149.93s/it, loss=0.811, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 35: val_loss reached 0.87480 (best 0.87480), saving model to \"c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=35-step=35.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 2/2 [05:00<00:00, 150.26s/it, loss=0.776, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 36: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 2/2 [05:04<00:00, 152.48s/it, loss=0.741, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 37: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 2/2 [05:03<00:00, 151.89s/it, loss=0.706, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 38: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 2/2 [05:03<00:00, 151.89s/it, loss=0.706, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:907: LightningDeprecationWarning: `trainer.test(test_dataloaders)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.test(dataloaders)` instead.\n",
      "  \"`trainer.test(test_dataloaders)` is deprecated in v1.4 and will be removed in v1.6.\"\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1399: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  f\"`.{fn}(ckpt_path=None)` was called without a model.\"\n",
      "Restoring states from the checkpoint path at c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=35-step=35.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from checkpoint at c:\\Users\\mcr\\Desktop\\sample\\checkpoints\\epoch=35-step=35.ckpt\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:27<00:00, 27.24s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 1/1 [00:27<00:00, 27.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks = [early_stop_callback, checkpoint_callback],\n",
    "    strategy = config.strategy,\n",
    "    logger = logger,\n",
    ")\n",
    "\n",
    "trainer.fit(bert_finetuner, train_dataloader, val_dataloader)\n",
    "trainer.test(test_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(config):\n",
    "    preprocessor = Preprocessor(config.max_len)\n",
    "    test_dataloader = get_dataloader(\n",
    "        config.test_data_path, preprocessor.get_input_features, config.eval_batch_size\n",
    "    )\n",
    "    model = SpacingBertModel(config, None, None, test_dataloader)\n",
    "    checkpoint = torch.load(config.ckpt_path, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    trainer = pl.Trainer()\n",
    "    res = trainer.test(model)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CorpusDataset object at 0x00000154E99C09C8>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000154E99C0F88>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1585: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  \"GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\"\n",
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcr\\.conda\\envs\\test\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: UNK seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_config = OmegaConf.load('C:/Users/mcr/Desktop/sample/eval_config.yaml')\n",
    "predict = testModel(e_config)\n",
    "\n",
    "predict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ca314b81ff11024e1d88600edb39d5575775917616c8d4d9e8dc26cc9e70e6f"
  },
  "kernelspec": {
   "display_name": "cp2-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
